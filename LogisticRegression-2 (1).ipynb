{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84be94b",
   "metadata": {},
   "source": [
    "ANS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240dda96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Precision and recall are two important metrics used to evaluate the performance of a classification model, particularly in binary classification problems.\\n\\nPrecision is a measure of how many of the predicted positive instances are actually positive. It is calculated as:\\n\\nPrecision = True Positives / (True Positives + False Positives)\\n\\nIn other words, precision measures the proportion of positive predictions that are actually correct. A high precision value indicates that the model has a low false positive rate, meaning that it rarely predicts positive instances when they are actually negative.\\n\\nRecall, on the other hand, is a measure of how many of the actual positive instances were correctly predicted as positive. It is calculated as:\\n\\nRecall = True Positives / (True Positives + False Negatives)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Precision and recall are two important metrics used to evaluate the performance of a classification model, particularly in binary classification problems.\n",
    "\n",
    "Precision is a measure of how many of the predicted positive instances are actually positive. It is calculated as:\n",
    "\n",
    "Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "In other words, precision measures the proportion of positive predictions that are actually correct. A high precision value indicates that the model has a low false positive rate, meaning that it rarely predicts positive instances when they are actually negative.\n",
    "\n",
    "Recall, on the other hand, is a measure of how many of the actual positive instances were correctly predicted as positive. It is calculated as:\n",
    "\n",
    "Recall = True Positives / (True Positives + False Negatives)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc0793",
   "metadata": {},
   "source": [
    "ANS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bba16d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The F1 score is a metric that combines precision and recall into a single value to provide a more comprehensive evaluation of a classification model's performance. It is the harmonic mean of precision and recall and is calculated as:\\n\\nF1 score = 2 * (precision * recall) / (precision + recall)\\n\\nThe F1 score ranges between 0 and 1, where a value of 1 indicates perfect precision and recall.\\n\\nThe F1 score is different from precision and recall in that it considers both metrics equally and is a balance between them. Precision and recall may have different values in certain situations, such as imbalanced datasets, where one metric may be inflated at the expense of the other. The F1 score is useful in such cases because it takes into account both metrics and provides a more accurate evaluation of the model's performance.\\n\\nFor example, suppose a model has high precision but low recall. This means that it rarely predicts positive instances incorrectly but may miss many actual positive instances. In contrast, a model with high recall but low precision may predict many positive instances, but a large proportion of them may be false positives. In both cases, the F1 score can help to provide a balanced evaluation of the model's performance by combining precision and recall into a single metric.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The F1 score is a metric that combines precision and recall into a single value to provide a more comprehensive evaluation of a classification model's performance. It is the harmonic mean of precision and recall and is calculated as:\n",
    "\n",
    "F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "The F1 score ranges between 0 and 1, where a value of 1 indicates perfect precision and recall.\n",
    "\n",
    "The F1 score is different from precision and recall in that it considers both metrics equally and is a balance between them. Precision and recall may have different values in certain situations, such as imbalanced datasets, where one metric may be inflated at the expense of the other. The F1 score is useful in such cases because it takes into account both metrics and provides a more accurate evaluation of the model's performance.\n",
    "\n",
    "For example, suppose a model has high precision but low recall. This means that it rarely predicts positive instances incorrectly but may miss many actual positive instances. In contrast, a model with high recall but low precision may predict many positive instances, but a large proportion of them may be false positives. In both cases, the F1 score can help to provide a balanced evaluation of the model's performance by combining precision and recall into a single metric.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c76ef",
   "metadata": {},
   "source": [
    "ANS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e73ce161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are commonly used metrics to evaluate the performance of classification models. ROC is a graphical representation of the performance of a binary classifier at different classification thresholds, while AUC is a numerical measure of the overall performance of the model.\\n\\nThe ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold values. TPR is the same as recall, or the proportion of true positive instances that are correctly identified by the model. FPR is the proportion of false positive instances that are incorrectly identified by the model. By varying the threshold value, we can create a curve that shows the trade-off between TPR and FPR.\\n\\nA perfect classifier will have an ROC curve that passes through the top left corner of the graph (TPR=1, FPR=0), indicating a high true positive rate and a low false positive rate. A random classifier will have an ROC curve that passes through the diagonal line (TPR=FPR), indicating that it is no better than random chance.\\n\\nAUC is a measure of the overall performance of the model, calculated as the area under the ROC curve. A perfect classifier will have an AUC of 1, while a random classifier will have an AUC of 0.5.\\n\\nROC and AUC are useful metrics for evaluating the performance of classification models because they are insensitive to the specific threshold chosen for classification, making them suitable for imbalanced datasets. Additionally, ROC and AUC can help to visualize the performance of the model at different levels of classification threshold and provide a more comprehensive evaluation of the model's performance than accuracy or other simple metrics.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are commonly used metrics to evaluate the performance of classification models. ROC is a graphical representation of the performance of a binary classifier at different classification thresholds, while AUC is a numerical measure of the overall performance of the model.\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at different threshold values. TPR is the same as recall, or the proportion of true positive instances that are correctly identified by the model. FPR is the proportion of false positive instances that are incorrectly identified by the model. By varying the threshold value, we can create a curve that shows the trade-off between TPR and FPR.\n",
    "\n",
    "A perfect classifier will have an ROC curve that passes through the top left corner of the graph (TPR=1, FPR=0), indicating a high true positive rate and a low false positive rate. A random classifier will have an ROC curve that passes through the diagonal line (TPR=FPR), indicating that it is no better than random chance.\n",
    "\n",
    "AUC is a measure of the overall performance of the model, calculated as the area under the ROC curve. A perfect classifier will have an AUC of 1, while a random classifier will have an AUC of 0.5.\n",
    "\n",
    "ROC and AUC are useful metrics for evaluating the performance of classification models because they are insensitive to the specific threshold chosen for classification, making them suitable for imbalanced datasets. Additionally, ROC and AUC can help to visualize the performance of the model at different levels of classification threshold and provide a more comprehensive evaluation of the model's performance than accuracy or other simple metrics.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d3cdb",
   "metadata": {},
   "source": [
    "ANS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e27002f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The choice of the best metric to evaluate the performance of a classification model depends on the specific problem at hand and the requirements of the stakeholders. Some metrics may be more important than others, depending on the application of the model. Here are some considerations when choosing the best metric:\\n\\nClass distribution: If the classes in the dataset are imbalanced, accuracy may not be the best metric since it may be biased towards the majority class. In this case, metrics such as precision, recall, and F1 score may provide a more accurate evaluation of the model's performance.\\n\\nCost of misclassification: The cost of misclassifying positive and negative instances may be different in certain applications. For example, in a medical diagnosis task, misclassifying a disease as non-disease may have different consequences than misclassifying non-disease as a disease. In such cases, metrics such as precision and recall may be more suitable for evaluating the model's performance.\\n\\nThreshold: The threshold for classification can affect the choice of the metric. For instance, if a high precision is required, a higher threshold may be used, while a lower threshold may be used for higher recall.\\n\\nBusiness goals: The ultimate goal of the classification model may vary depending on the business objective. For example, in an email spam detection task, the priority may be to minimize the number of false positives to avoid missing important emails. In such cases, precision may be more important than recall.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The choice of the best metric to evaluate the performance of a classification model depends on the specific problem at hand and the requirements of the stakeholders. Some metrics may be more important than others, depending on the application of the model. Here are some considerations when choosing the best metric:\n",
    "\n",
    "Class distribution: If the classes in the dataset are imbalanced, accuracy may not be the best metric since it may be biased towards the majority class. In this case, metrics such as precision, recall, and F1 score may provide a more accurate evaluation of the model's performance.\n",
    "\n",
    "Cost of misclassification: The cost of misclassifying positive and negative instances may be different in certain applications. For example, in a medical diagnosis task, misclassifying a disease as non-disease may have different consequences than misclassifying non-disease as a disease. In such cases, metrics such as precision and recall may be more suitable for evaluating the model's performance.\n",
    "\n",
    "Threshold: The threshold for classification can affect the choice of the metric. For instance, if a high precision is required, a higher threshold may be used, while a lower threshold may be used for higher recall.\n",
    "\n",
    "Business goals: The ultimate goal of the classification model may vary depending on the business objective. For example, in an email spam detection task, the priority may be to minimize the number of false positives to avoid missing important emails. In such cases, precision may be more important than recall.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8d58f",
   "metadata": {},
   "source": [
    "ANS5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f38c0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Logistic regression is a binary classification algorithm that predicts the probability of an instance belonging to a positive class. However, it can be extended to handle multiclass classification problems by using one of two approaches: one-vs-rest (OvR) or multinomial logistic regression (also known as softmax regression).\\n\\nOne-vs-rest (OvR) approach:\\nIn this approach, a separate logistic regression model is trained for each class against all the other classes. For instance, if there are three classes (A, B, and C), three separate logistic regression models are trained: one for A vs (B+C), one for B vs (A+C), and one for C vs (A+B). During prediction, the model with the highest probability is chosen as the predicted class.\\n\\nMultinomial logistic regression (softmax regression) approach:\\nIn this approach, a single model is trained to predict the probabilities for all the classes. The output layer of the model consists of multiple nodes, each representing a different class. The softmax activation function is used to convert the output of each node to a probability distribution over all the classes. During prediction, the class with the highest probability is chosen as the predicted class.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Logistic regression is a binary classification algorithm that predicts the probability of an instance belonging to a positive class. However, it can be extended to handle multiclass classification problems by using one of two approaches: one-vs-rest (OvR) or multinomial logistic regression (also known as softmax regression).\n",
    "\n",
    "One-vs-rest (OvR) approach:\n",
    "In this approach, a separate logistic regression model is trained for each class against all the other classes. For instance, if there are three classes (A, B, and C), three separate logistic regression models are trained: one for A vs (B+C), one for B vs (A+C), and one for C vs (A+B). During prediction, the model with the highest probability is chosen as the predicted class.\n",
    "\n",
    "Multinomial logistic regression (softmax regression) approach:\n",
    "In this approach, a single model is trained to predict the probabilities for all the classes. The output layer of the model consists of multiple nodes, each representing a different class. The softmax activation function is used to convert the output of each node to a probability distribution over all the classes. During prediction, the class with the highest probability is chosen as the predicted class.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41fa185",
   "metadata": {},
   "source": [
    "ANS6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f228fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  An end-to-end project for multiclass classification typically involves the following steps:\\n\\nProblem Definition:\\nThe first step is to define the problem statement and the goals of the project. This involves understanding the business problem, defining the input and output variables, and selecting the appropriate evaluation metric.\\n\\nData Collection:\\nThe next step is to collect the necessary data for the project. This involves identifying the relevant data sources, selecting the appropriate features, and ensuring that the data is representative of the problem at hand.\\n\\nData Preprocessing:\\nThe collected data is usually raw and unprocessed, and requires preprocessing to ensure that it is suitable for machine learning algorithms. This involves tasks such as data cleaning, feature scaling, feature selection, and handling missing data.\\n\\nData Visualization:\\nData visualization is an important step in understanding the data and identifying patterns and relationships between variables. This involves creating visualizations such as histograms, scatterplots, and heatmaps to explore the data.\\n\\nModel Selection:\\nThe next step is to select an appropriate machine learning algorithm for the problem at hand. This involves evaluating various models such as logistic regression, decision trees, random forests, and neural networks, and selecting the best model based on the evaluation metric.\\n\\nModel Training:\\nThe selected model is trained on the preprocessed data using techniques such as cross-validation to ensure that it generalizes well to new data.\\n\\nModel Evaluation:\\nThe trained model is evaluated on a holdout set or using cross-validation to estimate its performance on new data. This involves computing various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curve, and AUC.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  An end-to-end project for multiclass classification typically involves the following steps:\n",
    "\n",
    "Problem Definition:\n",
    "The first step is to define the problem statement and the goals of the project. This involves understanding the business problem, defining the input and output variables, and selecting the appropriate evaluation metric.\n",
    "\n",
    "Data Collection:\n",
    "The next step is to collect the necessary data for the project. This involves identifying the relevant data sources, selecting the appropriate features, and ensuring that the data is representative of the problem at hand.\n",
    "\n",
    "Data Preprocessing:\n",
    "The collected data is usually raw and unprocessed, and requires preprocessing to ensure that it is suitable for machine learning algorithms. This involves tasks such as data cleaning, feature scaling, feature selection, and handling missing data.\n",
    "\n",
    "Data Visualization:\n",
    "Data visualization is an important step in understanding the data and identifying patterns and relationships between variables. This involves creating visualizations such as histograms, scatterplots, and heatmaps to explore the data.\n",
    "\n",
    "Model Selection:\n",
    "The next step is to select an appropriate machine learning algorithm for the problem at hand. This involves evaluating various models such as logistic regression, decision trees, random forests, and neural networks, and selecting the best model based on the evaluation metric.\n",
    "\n",
    "Model Training:\n",
    "The selected model is trained on the preprocessed data using techniques such as cross-validation to ensure that it generalizes well to new data.\n",
    "\n",
    "Model Evaluation:\n",
    "The trained model is evaluated on a holdout set or using cross-validation to estimate its performance on new data. This involves computing various evaluation metrics such as accuracy, precision, recall, F1 score, ROC curve, and AUC.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e40e42a",
   "metadata": {},
   "source": [
    "ANS7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b96b101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Model deployment is the process of integrating a trained machine learning model into a production environment where it can be used to make predictions on new data. It involves setting up the necessary infrastructure, such as servers and APIs, to serve the model's predictions to end-users or downstream systems.\\n\\nModel deployment is important because it is the final step in the machine learning pipeline and determines the real-world impact of the model. A model that performs well in a development environment may not necessarily perform well in production, where it may encounter different data distributions, data quality issues, and performance constraints. Therefore, it is essential to carefully plan the deployment process to ensure that the model is accurate, reliable, and scalable.\\n\\nModel deployment also involves considerations around security, privacy, and compliance. Machine learning models often deal with sensitive data, and it is important to ensure that the model's predictions are not leaked or misused. In addition, regulations such as GDPR or HIPAA may impose restrictions on how data is stored, processed, and transmitted, which must be taken into account when deploying a model.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Model deployment is the process of integrating a trained machine learning model into a production environment where it can be used to make predictions on new data. It involves setting up the necessary infrastructure, such as servers and APIs, to serve the model's predictions to end-users or downstream systems.\n",
    "\n",
    "Model deployment is important because it is the final step in the machine learning pipeline and determines the real-world impact of the model. A model that performs well in a development environment may not necessarily perform well in production, where it may encounter different data distributions, data quality issues, and performance constraints. Therefore, it is essential to carefully plan the deployment process to ensure that the model is accurate, reliable, and scalable.\n",
    "\n",
    "Model deployment also involves considerations around security, privacy, and compliance. Machine learning models often deal with sensitive data, and it is important to ensure that the model's predictions are not leaked or misused. In addition, regulations such as GDPR or HIPAA may impose restrictions on how data is stored, processed, and transmitted, which must be taken into account when deploying a model.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2ee40",
   "metadata": {},
   "source": [
    "ANS8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6784aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Multi-cloud platforms are used for model deployment to provide a flexible and scalable infrastructure for running machine learning models. A multi-cloud platform allows users to deploy their models on multiple cloud providers, such as Amazon Web Services, Microsoft Azure, or Google Cloud Platform, to take advantage of each provider's unique features and capabilities.\\n\\nOne advantage of a multi-cloud platform is that it allows users to choose the best cloud provider for their specific use case, based on factors such as cost, performance, and security. For example, a user may choose to deploy their model on AWS for its low-cost storage and compute resources, but use Google Cloud Platform for its advanced machine learning tools and services.\\n\\nAnother advantage of a multi-cloud platform is that it provides redundancy and resilience, by distributing the workload across multiple cloud providers. This can help to minimize downtime and ensure that the model is always available, even if one provider experiences an outage or performance issue.\\n\\nTo deploy a machine learning model on a multi-cloud platform, the user typically needs to follow a few steps:\\n\\nCreate an account and set up the necessary credentials for each cloud provider they want to use.\\nDevelop and train the machine learning model using their preferred framework and tools, such as TensorFlow, PyTorch, or Scikit-learn.\\nPackage the model into a container, such as Docker, that can be easily deployed on a cloud platform.\\nChoose the cloud providers and regions where they want to deploy the model, based on factors such as latency, cost, and compliance requirements.\\nDeploy the containerized model on each cloud provider, using their respective deployment tools and services, such as Kubernetes, AWS Lambda, or Google Cloud Functions.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Multi-cloud platforms are used for model deployment to provide a flexible and scalable infrastructure for running machine learning models. A multi-cloud platform allows users to deploy their models on multiple cloud providers, such as Amazon Web Services, Microsoft Azure, or Google Cloud Platform, to take advantage of each provider's unique features and capabilities.\n",
    "\n",
    "One advantage of a multi-cloud platform is that it allows users to choose the best cloud provider for their specific use case, based on factors such as cost, performance, and security. For example, a user may choose to deploy their model on AWS for its low-cost storage and compute resources, but use Google Cloud Platform for its advanced machine learning tools and services.\n",
    "\n",
    "Another advantage of a multi-cloud platform is that it provides redundancy and resilience, by distributing the workload across multiple cloud providers. This can help to minimize downtime and ensure that the model is always available, even if one provider experiences an outage or performance issue.\n",
    "\n",
    "To deploy a machine learning model on a multi-cloud platform, the user typically needs to follow a few steps:\n",
    "\n",
    "Create an account and set up the necessary credentials for each cloud provider they want to use.\n",
    "Develop and train the machine learning model using their preferred framework and tools, such as TensorFlow, PyTorch, or Scikit-learn.\n",
    "Package the model into a container, such as Docker, that can be easily deployed on a cloud platform.\n",
    "Choose the cloud providers and regions where they want to deploy the model, based on factors such as latency, cost, and compliance requirements.\n",
    "Deploy the containerized model on each cloud provider, using their respective deployment tools and services, such as Kubernetes, AWS Lambda, or Google Cloud Functions.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c003c38a",
   "metadata": {},
   "source": [
    "ANS9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b6abf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Deploying machine learning models in a multi-cloud environment has both benefits and challenges.\\n\\nBenefits:\\n\\nFlexibility: Multi-cloud platforms offer flexibility, allowing businesses to choose the best cloud provider for each use case based on factors such as cost, performance, and security.\\nResilience: Multi-cloud platforms provide redundancy, allowing models to run across multiple cloud providers. This minimizes downtime and ensures that the model is always available, even if one provider experiences an outage or performance issue.\\nScalability: Multi-cloud platforms can provide scalability, allowing businesses to rapidly scale their models to meet changing demands.\\nChallenges:\\n\\nComplexity: Managing and deploying models across multiple cloud providers can be complex and require specialized expertise, which can add overhead to an organization's resources and slow down deployment timelines.\\nData Governance: Data governance can be challenging in a multi-cloud environment. Data may be located in different cloud providers, making it difficult to ensure compliance with regulations such as GDPR or HIPAA.\\nInteroperability: Cloud providers may have different APIs, making it challenging to create interoperable models that can be deployed across multiple providers.\\nCost: While multi-cloud platforms can offer cost savings, they can also add to the complexity of managing cloud resources, and businesses may need to pay for additional services to manage and deploy their models across multiple providers.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Deploying machine learning models in a multi-cloud environment has both benefits and challenges.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "Flexibility: Multi-cloud platforms offer flexibility, allowing businesses to choose the best cloud provider for each use case based on factors such as cost, performance, and security.\n",
    "Resilience: Multi-cloud platforms provide redundancy, allowing models to run across multiple cloud providers. This minimizes downtime and ensures that the model is always available, even if one provider experiences an outage or performance issue.\n",
    "Scalability: Multi-cloud platforms can provide scalability, allowing businesses to rapidly scale their models to meet changing demands.\n",
    "Challenges:\n",
    "\n",
    "Complexity: Managing and deploying models across multiple cloud providers can be complex and require specialized expertise, which can add overhead to an organization's resources and slow down deployment timelines.\n",
    "Data Governance: Data governance can be challenging in a multi-cloud environment. Data may be located in different cloud providers, making it difficult to ensure compliance with regulations such as GDPR or HIPAA.\n",
    "Interoperability: Cloud providers may have different APIs, making it challenging to create interoperable models that can be deployed across multiple providers.\n",
    "Cost: While multi-cloud platforms can offer cost savings, they can also add to the complexity of managing cloud resources, and businesses may need to pay for additional services to manage and deploy their models across multiple providers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae15d947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
