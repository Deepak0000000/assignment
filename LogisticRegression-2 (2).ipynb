{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a36562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "har har mahadev\n"
     ]
    }
   ],
   "source": [
    "print(\"har har mahadev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f437f60",
   "metadata": {},
   "source": [
    "ANS1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9fe4818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The theorem mathematically describes the relationship between the conditional probabilities of two events. Let's denote two events as A and B. Bayes' theorem states that the probability of event A occurring given that event B has occurred (denoted as P(A|B)) can be calculated using the following formula:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nHere's a breakdown of the terms:\\n\\nP(A|B) represents the probability of event A occurring given that event B has occurred. This is known as the posterior probability.\\nP(B|A) is the probability of event B occurring given that event A has occurred. It's called the likelihood.\\nP(A) is the probability of event A occurring. It's known as the prior probability.\\nP(B) represents the probability of event B occurring. It acts as a normalization factor.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" The theorem mathematically describes the relationship between the conditional probabilities of two events. Let's denote two events as A and B. Bayes' theorem states that the probability of event A occurring given that event B has occurred (denoted as P(A|B)) can be calculated using the following formula:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Here's a breakdown of the terms:\n",
    "\n",
    "P(A|B) represents the probability of event A occurring given that event B has occurred. This is known as the posterior probability.\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred. It's called the likelihood.\n",
    "P(A) is the probability of event A occurring. It's known as the prior probability.\n",
    "P(B) represents the probability of event B occurring. It acts as a normalization factor.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f5d79",
   "metadata": {},
   "source": [
    "ANS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e87d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" P(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nIn this formula:\\n\\nP(A|B) represents the probability of event A occurring given that event B has occurred. This is called the posterior probability.\\nP(B|A) is the probability of event B occurring given that event A has occurred. It's known as the likelihood.\\nP(A) is the probability of event A occurring. It's referred to as the prior probability.\\nP(B) represents the probability of event B occurring. It acts as a normalization factor.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula:\n",
    "\n",
    "P(A|B) represents the probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred. It's known as the likelihood.\n",
    "P(A) is the probability of event A occurring. It's referred to as the prior probability.\n",
    "P(B) represents the probability of event B occurring. It acts as a normalization factor.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0dcb3",
   "metadata": {},
   "source": [
    "ANS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88c5b801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Bayes' theorem is widely used in various fields to make probabilistic inferences and update beliefs based on new evidence. Here are a few practical applications of Bayes' theorem:\\n\\nMedical Diagnosis: Bayes' theorem is employed in medical diagnosis to assess the probability of a patient having a particular disease based on observed symptoms or test results. By combining prior probabilities (prevalence of the disease) with the likelihood of observing those symptoms given the disease, doctors can update their belief and provide a more accurate diagnosis.\\n\\nSpam Filtering: Email spam filters often utilize Bayes' theorem to classify incoming emails as spam or legitimate. The algorithm learns from a labeled dataset and calculates the probability of an email being spam given certain features (e.g., specific words, email headers). Bayes' theorem allows the filter to update the probability based on observed features and make accurate spam classification decisions.\\n\\nWeather Forecasting: Weather forecasting models employ Bayesian techniques to continuously update predictions based on observed weather conditions. By incorporating prior knowledge about weather patterns, historical data, and current measurements, meteorologists can use Bayes' theorem to refine their predictions and improve the accuracy of weather forecasts.\\n\\nMachine Learning: Bayesian inference is a powerful technique in machine learning. Bayesian models can make predictions by updating prior beliefs about model parameters based on observed data. Bayesian networks and Bayesian classifiers are examples of machine learning models that apply Bayes' theorem to model dependencies and make probabilistic predictions.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Bayes' theorem is widely used in various fields to make probabilistic inferences and update beliefs based on new evidence. Here are a few practical applications of Bayes' theorem:\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is employed in medical diagnosis to assess the probability of a patient having a particular disease based on observed symptoms or test results. By combining prior probabilities (prevalence of the disease) with the likelihood of observing those symptoms given the disease, doctors can update their belief and provide a more accurate diagnosis.\n",
    "\n",
    "Spam Filtering: Email spam filters often utilize Bayes' theorem to classify incoming emails as spam or legitimate. The algorithm learns from a labeled dataset and calculates the probability of an email being spam given certain features (e.g., specific words, email headers). Bayes' theorem allows the filter to update the probability based on observed features and make accurate spam classification decisions.\n",
    "\n",
    "Weather Forecasting: Weather forecasting models employ Bayesian techniques to continuously update predictions based on observed weather conditions. By incorporating prior knowledge about weather patterns, historical data, and current measurements, meteorologists can use Bayes' theorem to refine their predictions and improve the accuracy of weather forecasts.\n",
    "\n",
    "Machine Learning: Bayesian inference is a powerful technique in machine learning. Bayesian models can make predictions by updating prior beliefs about model parameters based on observed data. Bayesian networks and Bayesian classifiers are examples of machine learning models that apply Bayes' theorem to model dependencies and make probabilistic predictions.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1d0c44",
   "metadata": {},
   "source": [
    "ANS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12352dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the definition of conditional probability.\\n\\nConditional probability is the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B). It represents the updated probability of event A based on the information provided by event B.\\n\\nBayes' theorem provides a mathematical formula to calculate the conditional probability P(A|B) based on the known probabilities of P(B|A), P(A), and P(B). The formula is as follows:\\n\\nP(A|B) = (P(B|A) * P(A)) / P(B)\\n\\nIn this formula, P(B|A) represents the probability of event B occurring given that event A has occurred, P(A) is the probability of event A occurring (prior probability), and P(B) is the probability of event B occurring.\\n\\nBayes' theorem allows us to update our prior beliefs (prior probability) about event A based on the observed occurrence of event B. It provides a systematic way of incorporating new evidence (likelihood) into our existing beliefs and calculating the updated probability (posterior probability).\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "\n",
    "Conditional probability is the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B). It represents the updated probability of event A based on the information provided by event B.\n",
    "\n",
    "Bayes' theorem provides a mathematical formula to calculate the conditional probability P(A|B) based on the known probabilities of P(B|A), P(A), and P(B). The formula is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula, P(B|A) represents the probability of event B occurring given that event A has occurred, P(A) is the probability of event A occurring (prior probability), and P(B) is the probability of event B occurring.\n",
    "\n",
    "Bayes' theorem allows us to update our prior beliefs (prior probability) about event A based on the observed occurrence of event B. It provides a systematic way of incorporating new evidence (likelihood) into our existing beliefs and calculating the updated probability (posterior probability).\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337de9e3",
   "metadata": {},
   "source": [
    "ANS5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5021f11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" When choosing which type of Naive Bayes classifier to use for a given problem, it's important to consider the characteristics of the problem and the assumptions made by each variant of the classifier. The three common variants of Naive Bayes classifiers are Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's a breakdown of each variant and their typical use cases:\\n\\nGaussian Naive Bayes:\\n\\nAssumption: Assumes that the features follow a Gaussian (normal) distribution.\\nUse Case: It is suitable for continuous numerical features.\\nExample Application: Predicting the class of an email as spam or not spam based on features such as the length of the email, the number of links, etc.\\nMultinomial Naive Bayes:\\n\\nAssumption: Assumes that the features are discrete and follow a multinomial distribution.\\nUse Case: It is commonly used for text classification problems, where the features are word frequencies or occurrence counts.\\nExample Application: Sentiment analysis, document classification, spam filtering based on word frequencies.\\nBernoulli Naive Bayes:\\n\\nAssumption: Assumes that the features are binary or boolean (0s and 1s).\\nUse Case: It is appropriate for binary feature vectors or presence/absence features.\\nExample Application: Text classification, spam filtering based on the presence or absence of certain words or features.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" When choosing which type of Naive Bayes classifier to use for a given problem, it's important to consider the characteristics of the problem and the assumptions made by each variant of the classifier. The three common variants of Naive Bayes classifiers are Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's a breakdown of each variant and their typical use cases:\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features follow a Gaussian (normal) distribution.\n",
    "Use Case: It is suitable for continuous numerical features.\n",
    "Example Application: Predicting the class of an email as spam or not spam based on features such as the length of the email, the number of links, etc.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features are discrete and follow a multinomial distribution.\n",
    "Use Case: It is commonly used for text classification problems, where the features are word frequencies or occurrence counts.\n",
    "Example Application: Sentiment analysis, document classification, spam filtering based on word frequencies.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Assumption: Assumes that the features are binary or boolean (0s and 1s).\n",
    "Use Case: It is appropriate for binary feature vectors or presence/absence features.\n",
    "Example Application: Text classification, spam filtering based on the presence or absence of certain words or features.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28ff51a",
   "metadata": {},
   "source": [
    "ANS6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f760d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" First, let's calculate the class priors. Since equal prior probabilities are assumed for each class, both class A and class B have a prior probability of 0.5.\\n\\nNext, we calculate the likelihoods for each feature value given each class. For example, to calculate the likelihood of X1=3 given class A, we count the number of occurrences of X1=3 when class A is true, which is 4. We then divide this count by the total number of occurrences of class A, which is 17. Similarly, we calculate the likelihoods for other feature values and classes.\\n\\nClass A:\\nP(X1=3|A) = 4/17\\nP(X2=4|A) = 3/17\\n\\nClass B:\\nP(X1=3|B) = 1/10\\nP(X2=4|B) = 3/10\\n\\nNow, we can calculate the posterior probabilities using Bayes' theorem. Since the prior probabilities for both classes are equal, we can ignore them in the calculation and compare only the likelihoods.\\n\\nP(A|X1=3, X2=4) ∝ P(X1=3|A) * P(X2=4|A)\\n= (4/17) * (3/17)\\n≈ 0.042\\n\\nP(B|X1=3, X2=4) ∝ P(X1=3|B) * P(X2=4|B)\\n= (1/10) * (3/10)\\n= 0.03\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" First, let's calculate the class priors. Since equal prior probabilities are assumed for each class, both class A and class B have a prior probability of 0.5.\n",
    "\n",
    "Next, we calculate the likelihoods for each feature value given each class. For example, to calculate the likelihood of X1=3 given class A, we count the number of occurrences of X1=3 when class A is true, which is 4. We then divide this count by the total number of occurrences of class A, which is 17. Similarly, we calculate the likelihoods for other feature values and classes.\n",
    "\n",
    "Class A:\n",
    "P(X1=3|A) = 4/17\n",
    "P(X2=4|A) = 3/17\n",
    "\n",
    "Class B:\n",
    "P(X1=3|B) = 1/10\n",
    "P(X2=4|B) = 3/10\n",
    "\n",
    "Now, we can calculate the posterior probabilities using Bayes' theorem. Since the prior probabilities for both classes are equal, we can ignore them in the calculation and compare only the likelihoods.\n",
    "\n",
    "P(A|X1=3, X2=4) ∝ P(X1=3|A) * P(X2=4|A)\n",
    "= (4/17) * (3/17)\n",
    "≈ 0.042\n",
    "\n",
    "P(B|X1=3, X2=4) ∝ P(X1=3|B) * P(X2=4|B)\n",
    "= (1/10) * (3/10)\n",
    "= 0.03\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd754a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
